{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "######    \n",
    "#    This is a demo of text classification using the \"bag of words\" technique.\n",
    "#    The classifier used is logistic but the same can be implemented with Support Vector Machine.\n",
    "#    The code utilizes TensorFlow and will be run on a CPU and a GPU. The resulting training time will be recorded.\n",
    "#    We would expect a much better traning performance on GPU.\n",
    "#    \n",
    "#\n",
    "#    @author: Harshal Patil\n",
    "######    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named pandas",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a40e39eb9681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named pandas"
     ]
    }
   ],
   "source": [
    "#     load packages\n",
    "import os \n",
    "import zipfile\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "import time \n",
    "import matplotlib\n",
    "#matplotlib.use(\"MacOSX\")\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     load the data\n",
    "#     the original file is at https://archive.ics.uci.edu/ml/machine-learning-databases/00228/\n",
    "#     it is downloaded and saved locally\n",
    "#     the location is the current working directory which is not changed in this code\n",
    "cwd = os.getcwd()\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "urllib.urlretrieve(url, \"smsspamcollection.zip\")\n",
    "dnldf = os.path.join(cwd,\"smsspamcollection.zip\")\n",
    "zipfile.ZipFile(dnldf).extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#     open the text file\n",
    "filelocation = os.path.join(cwd,\"SMSSpamCollection\")\n",
    "\n",
    "#    parse the file\n",
    "#    first column has label - Spam or Ham and second column has the message\n",
    "#     read into a pandas dataframe\n",
    "df = pd.read_csv(filelocation,sep=\"\\t\",names=[\"class\",\"sms\"])\n",
    "df[\"label\"] = np.vectorize(lambda x: 1 if(x==\"spam\") else 0)(df.loc[:,\"class\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    check proper coding of label from class\n",
    "print \"1. check counts and percentages \\n\"\n",
    "print pd.crosstab(df[\"label\"],df[\"class\"],margins=True)\n",
    "print \"\\n\"\n",
    "\n",
    "#    check balance between positive and negative cases\n",
    "#    check the percentage of positive cases \n",
    "print pd.crosstab(df[\"label\"],df[\"class\"],margins=True).apply(lambda x: x/len(df), axis=1)\n",
    "print \"\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    preprocess strings to normalize and improve model performance\n",
    "#    conver to lowercase, remove punctuations, numbers and extra spaces, tabs etc.\n",
    "#    No stopwords removal - although it can be tried\n",
    "message = np.vectorize(lambda x: x.lower())(df[\"sms\"])\n",
    "message = np.vectorize(lambda x: re.sub(\"[\"+string.punctuation+\"]\",\" \",x))(message)\n",
    "message = np.vectorize(lambda x: re.sub(\"[0123456789]\",\" \",x))(message)\n",
    "message = np.vectorize(lambda x: \" \".join(x.split()))(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    we will create a documentxterm matrix which will be passed to learner\n",
    "#    The counts will be using TF-IDF to differentiate unique terms and add higher weight to them\n",
    "#    you may need to run nltk.download()\n",
    "tfidfconvert = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "termdoc = tfidfconvert.fit_transform(message)\n",
    "\n",
    "print \"2. Document x Term Matrix size : \"+ str(termdoc.shape[0])+\" rows by \"+ str(termdoc.shape[1])+\" columns \\n\"\n",
    "print \"columns indicate the specific words and as suspected there are too many\"\n",
    "print \"so we would reduce the size a bit arbitrarily to top 3000 words\"\n",
    "\n",
    "tfidfconvert = TfidfVectorizer(tokenizer=nltk.word_tokenize,max_features=3000)\n",
    "termdoc = tfidfconvert.fit_transform(message)\n",
    "print \"3. Document x Term Matrix size : \"+ str(termdoc.shape[0])+\" rows by \"+ str(termdoc.shape[1])+\" columns \\n\"\n",
    "\n",
    "nrow = termdoc.shape[0]\n",
    "ncol = termdoc.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    divide in training and test set \n",
    "train_set = np.random.choice(nrow,int(round(0.8*nrow)),replace=False)\n",
    "test_set = np.delete(range(nrow),train_set)\n",
    "\n",
    "print \"4. Training Set Size : \" + str(len(train_set))\n",
    "print \"5. Test Set Size : \" + str(len(test_set))\n",
    "print \"\\n Establish TensorFlow graph and Train the model \\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# actual training and test data\n",
    "matrix_train = termdoc[train_set] \n",
    "matrix_test = termdoc[test_set] \n",
    "target_train = np.transpose([df[\"label\"][train_set]])\n",
    "target_test = np.transpose([df[\"label\"][test_set]])\n",
    "\n",
    "#matrix_train.shape\n",
    "#matrix_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    build tensorflow graph\n",
    "#    We will use logistic model with TensorFlow\n",
    "#    z=WX+b where W is a weight vector and X is input data matrix and b is the bias \n",
    "#    probability = exp(z)/(1+exp(z))\n",
    "#    we will initialize the weight vector and bias to random normal values\n",
    "\n",
    "#specify device - we will change this to GPU to check difference\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "#with tf.device('/cpu:0'):\n",
    "W = tf.Variable(tf.random_normal(shape=[ncol,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "#    placeholder for data input - shape is none where we dont know in advance\n",
    "x_data = tf.placeholder(shape=[None, ncol], dtype=tf.float32)\n",
    "y_target = tf.placeholder(shape=[None, 1], dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    Model output and loss function\n",
    "#with tf.device('/cpu:0'):\n",
    "with tf.device(\"/gpu:0\"):\n",
    "\tmodel_output = tf.add(tf.matmul(x_data, W), b)\n",
    "\tloss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "\n",
    "#    we want high probability for the class=1\n",
    "\tprediction = tf.round(tf.sigmoid(model_output))\n",
    "\tpredictions_correct = tf.cast(tf.equal(prediction, y_target), tf.float32)\n",
    "\taccuracy = tf.reduce_mean(predictions_correct)\n",
    "\n",
    "#    declare which optimizer to use and minimize loss - learning rate is provided\n",
    "\tmy_opt = tf.train.GradientDescentOptimizer(0.0025)\n",
    "\ttrain_step = my_opt.minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    Intitialize Variables\n",
    "#    session is created to log deployment details\n",
    "\n",
    "# when GPU is present and want to check deployment to GPU\n",
    "#sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#    training the model - we go through a number of random batches to train\n",
    "batch=200\n",
    "rounds = 10000\n",
    "\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "i_data = []\n",
    "runtime = []\n",
    "i=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "for i in range(rounds):\n",
    "    rand_index = np.random.choice(matrix_train.shape[0], size=batch)\n",
    "    rand_x = matrix_train[rand_index].todense() \n",
    "    #rand_y = np.transpose([target_train[rand_index]])\n",
    "    rand_y = target_train[rand_index]\n",
    "    sess.run(train_step, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "    \n",
    "    # Only record loss and accuracy every 100 generations\n",
    "    if (i+1)%100==0:\n",
    "        i_data.append(i+1)\n",
    "        t1 = time.time()\n",
    "        train_loss_temp = sess.run(loss, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "        t2=time.time()\n",
    "        train_loss.append(train_loss_temp)\n",
    "        runtime.append(t2-t1)\n",
    "        \n",
    "        t1 = time.time()\n",
    "        test_loss_temp = sess.run(loss, feed_dict={x_data: matrix_test.todense(), y_target: target_test})\n",
    "        t2=time.time()\n",
    "        test_loss.append(test_loss_temp)\n",
    "        runtime.append(t2-t1)\n",
    "         \n",
    "        train_acc_temp = sess.run(accuracy, feed_dict={x_data: rand_x, y_target: rand_y})\n",
    "        train_acc.append(train_acc_temp)\n",
    "    \n",
    "        test_acc_temp = sess.run(accuracy, feed_dict={x_data: matrix_test.todense(), y_target: target_test})\n",
    "        test_acc.append(test_acc_temp)\n",
    "    if (i+1)%500==0:\n",
    "        acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp, test_acc_temp]\n",
    "        acc_and_loss = [np.round(x,2) for x in acc_and_loss]\n",
    "        print('Generation # {}. Train Loss (Test Loss): {:.2f} ({:.2f}). Train Acc (Test Acc): {:.2f} ({:.2f})'.format(*acc_and_loss))\n",
    "\n",
    "time_end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"\\n 6. Training time in seconds (total) \" + str(time_end - time_start)\n",
    "print \"7. Training time in seconds (only for running Loss node in TensorFlow graph) \" + str(sum(runtime))\n",
    "\n",
    "print \"\\n 8. Loss Function\"\n",
    "plt.plot(i_data,train_loss,label=\"Training Set\")\n",
    "plt.plot(i_data,test_loss,label=\"Test Set\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print \"9. Accuracy\"\n",
    "plt.plot(i_data,train_acc,label=\"Training Set\")\n",
    "plt.plot(i_data,test_acc,label=\"Test Set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
